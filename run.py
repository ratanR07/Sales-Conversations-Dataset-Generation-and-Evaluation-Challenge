# -*- coding: utf-8 -*-
"""Untitled.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kAMMsr3eGBRnxyFXfv18grttKaIOVbec
"""

# Import necessary libraries
import pandas as pd
from datetime import datetime
import random
import time
import torch
import re
from transformers import GPT2LMHeadModel, GPT2Tokenizer

# Set CUDA_LAUNCH_BLOCKING for debugging
import os
os.environ['CUDA_LAUNCH_BLOCKING'] = "1"

# Check if GPU is available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Load pre-trained model and tokenizer
model_name = "gpt2-medium"
model = GPT2LMHeadModel.from_pretrained(model_name).to(device)
tokenizer = GPT2Tokenizer.from_pretrained(model_name)

# Set the pad_token_id to the eos_token_id to avoid warnings
model.config.pad_token_id = model.config.eos_token_id

# Maximum number of tokens for the model
MAX_LENGTH = 1024
NEW_TOKENS = 100

# Function to generate sales conversation
def generate_sales_conversation(prompt):
    try:
        inputs = tokenizer(prompt, return_tensors='pt').to(device)
        outputs = model.generate(
            inputs['input_ids'],
            max_new_tokens=NEW_TOKENS,
            temperature=0.7,
            do_sample=True,
            attention_mask=inputs['attention_mask']
        )
        conversation = tokenizer.decode(outputs[0], skip_special_tokens=True)
        return conversation.strip()
    except Exception as e:
        print(f"Error during generation: {e}")
        return ""

# Function to split the conversation into sentences
def split_into_sentences(text):
    # Use regular expressions to split the text into sentences
    sentence_endings = re.compile(r'(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=\.|\?)\s')
    sentences = sentence_endings.split(text)
    return sentences

# Generate dialogues
num_conversations = 2
num_exchanges = 10  # Number of exchanges to generate in each conversation
conversations = []

for i in range(num_conversations):
    initial_prompt = """
    Salesman: Hello! I'm excited to introduce you to our latest product. It’s designed to help you manage your tasks more efficiently. How do you currently manage your tasks?
    Customer: I use a basic to-do list app, but it lacks some features I need.
    Salesman: Our product offers advanced features like task prioritization, reminders, and collaboration tools. Would you be interested in a demo?
    Customer: Yes, that sounds great!
    Salesman: Great! I’ll schedule a demo for you. In the meantime, you can check out our website for more information.
    Salesman: Can you tell me more about the features you are looking for?
    Customer: Well, I need features that help me track my progress and collaborate with my team.
    Salesman: We have just the solution for you! Our product allows real-time collaboration and provides detailed progress tracking.
    Customer: That sounds exactly like what I need. How do I get started?
    Salesman: You can start by signing up on our website. I can walk you through the process.
    Customer: Thank you! That would be very helpful.
    Salesman: My pleasure! Let’s get you set up. Is there anything else you’d like to know?
    Customer: Not at the moment. I’m excited to try this out.
    Salesman: Fantastic! I’ll be here if you need any further assistance.
    """
    prompt = initial_prompt

    for _ in range(num_exchanges):
        # Ensure the prompt length is within the maximum length
        prompt_tokens = tokenizer(prompt, return_tensors='pt')['input_ids']
        if prompt_tokens.size(1) > MAX_LENGTH - NEW_TOKENS:
            # Trim the oldest part of the conversation
            excess_length = prompt_tokens.size(1) - (MAX_LENGTH - NEW_TOKENS)
            prompt = tokenizer.decode(prompt_tokens[0, excess_length:], skip_special_tokens=True)

        generated_text = generate_sales_conversation(prompt)
        if not generated_text:
            break

        dialogues = split_into_sentences(generated_text)

        for dialogue in dialogues:
            dialogue = dialogue.strip()
            if dialogue.startswith("Salesman:") or dialogue.startswith("Customer:"):
                if ': ' in dialogue:
                    speaker, response = dialogue.split(': ', 1)
                    conversations.append([speaker, response, datetime.now().strftime('%Y-%m-%d %H:%M:%S')])
                    prompt += f"{speaker}: {response}\n"
                else:
                    print(f"Skipping malformed dialogue: {dialogue}")
            else:
                print(f"Skipping non-dialogue text: {dialogue}")

        print(f"Generated {_ + 1}/{num_exchanges} exchanges for conversation {i + 1}/{num_conversations}")
        time.sleep(random.uniform(1, 3))  # Adding delay to mimic natural conversation flow

# Save the dataset to CSV
df = pd.DataFrame(conversations, columns=['Speaker', 'Response', 'Timestamp'])
df.to_csv('sales_conversations.csv', index=False)

print("Dataset generation complete. Saved to 'sales_conversations.csv'.")

df

df = pd.DataFrame(df)

# Save DataFrame to CSV
df.to_csv('D:\AI task\df.csv', index=False)

